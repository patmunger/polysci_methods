---
title: "Week 10 - Count Models"
author:
- Ozlem Tuncel
- updated by Patrick Munger
date: "Fall 2024"
output:
  html_document:
    df_print: paged
urlcolor: blue
---

# Today's agenda

1.  Estimate Poisson Model with GLM
2.  Test for overdispersion
3.  Estimate Negative Binomial with GLM
4.  Estimate zero-inflation models
5.  Plot Marginal Effects

# Preliminaries

```{r directory, eval=FALSE, message=FALSE, warning=FALSE}
getwd() # Check your current working directory
# setwd() # Set and specify your working directory to the desired folder
```

```{r set_seed, eval=FALSE, message=FALSE, warning=FALSE}
# Specify any integer
set.seed(1234)
```

```{r}
library(tidyverse) ## Utility tools
library(stargazer) ## Tables
library(car) # Companion to Applied Regression
library(carData) # Suppleme Data
library(gmodels) # Crosstabs
library(sandwich) # Robust standard errors
library(lmtest)
library(MASS) # glm.nb
library(pscl) # zeroinfl
library(AER)
library(ggeffects)
```

# Upload data

```{r}
load("terrorism_data.Rdata")
```

We are going to run several models. The dependent variable is the number of victims for bombing and explosions from terrorism data (Global Terrorism Database, 2000-2019).

# Count models

Count models are used for dependent variables in which each observation represents the number of times an event occurred within that temporal/spatial unit. Count variables can be modeled with OLS, but are ofter better modeled with distributions other than the normal, since count variables cannot take on negative values, and their distributions are often concentrated towards lower counts.

Count models are typically modeled with either the Poisson or Negative Binomial distribution:

-   Poisson Model – assumes "equidispersion" represented by the conditional mean equalling the conditional variance. Also assumed independence of observations (occurance of an event should not be correlated with occurance of future events). In practice, these assumptions are often violated.

-   Negative binomial regression – Accounts for overdispersion (conditional mean not equalling conditional variance) with inclusion of a dispersion parameter (alpha).

Another issue with in count models, is the poor fit of a model when there are a high nimber of xeros (observations where no events occurred). Standard Poisson and Negative Binomial models tend to greatly underpredict these zeros. One solution is the zero-inflated count model:

-   The xero-inflated count model introduces a two-stage modeling process first outcomes that are never zero or not never zero are modeled with a logit or probit. Then the outcomes that are not never (but still can be zero) are modeled with Poisson or Negative Binomial.

# Poisson Model with GLM

The `glm` syntax is as usual, with the `family` argument set to `"poisson"`.

-   In calling the data, we subset the `type_of_attack` variable to only include bombing/explosion attacks, assuming this is our theoretical interest.

-   Note on the GDP variable: Scale function subtracts the values of each column by the matching “center” value from the argument. $xscaled = (x – mean_x) / s$ where: `x` is the real x-value, `mean_x` is the Sample mean, and `s` is the sample SD. This is also known as data standardization, and it basically involves converting each original value into a z-score. It is used to improve convergence in models that are sensitive to the scale of certain predictors with a high variance like GDP.

```{r}
# Using GLM function
p1 <- glm(number_of_victims ~ number_of_perpetrators + scale(GDP) + 
                      GDP_Growth + Trade_Perc_GDP + Mineral_Rents_Perc, 
                      data = combo.df[combo.df$type_of_attack == 3,], 
                      # subset: 3 = Bombing/Explosion
                      family = "poisson") 

summary(p1)
stargazer(p1, type = "text")
```

Recall that the dependent variable is a count variable, and Poisson regression models the log of the expected count as a function of the predictor variables. We can interpret the Poisson regression coefficient as follows: for a one unit change in the predictor variable, the difference in the logs of expected counts is expected to change by the respective regression coefficient, given the other predictor variables in the model are held constant.

The first column named Estimate is the coefficient values of $\alpha$ (intercept), $\beta_1$ and so on. Following is the interpretation for the parameter estimates:

1.  exp($\alpha$)= effect on the mean $\mu$, when X = 0

2.  exp($\beta$) = with every unit increase in X, the predictor variable has multiplicative effect of exp($\beta$) on the mean of Y, that is $\mu$.

3.  If $\beta$ = 0, then exp($\beta$) = 1, and the expected count is exp($\alpha$) and, Y and X are not related.

4.  If $\beta$ \> 0, then exp($\beta$) \> 1, and the expected count is exp($\beta$) times larger than when X = 0

5.  If $\beta$ \< 0, then exp($\beta$) \< 1, and the expected count is exp($\beta$) times smaller than when X = 0

# Interpreting p1 model

-   The coefficient for `number_of_perpetrators` is 0.006, which means that for each additional perpetrator, the expected log count of victims increases by 0.006.

-   The coefficient for `Mineral_Rents_Perc` is -0.529, meaning that a one-unit increase in `Mineral_Rents_Perc,` the expected log count of victims decreases by 0.529.

We can exponentiate these coefficients for more intuitive reading.

```{r}
# Substantive interpretation
exp(p1$coefficients)  
```

-   The exponentiated coefficient for `number_of_perpetrators` is 1.006, which means that each additional perpetrator is associated with a **0.6% increase** in the expected count of victims ( calculated as $(exp(coefficent)-1)*100$).

-   The exponentiated coefficient for `Mineral_Rents_Perc` is 0.59, meaning that a one-unit increase in `Mineral_Rents_Perc` is associated with a **41% decrease** in the expected count of victims (calculated as $(exp(coefficent)-1)*100$).

# Dispersion test

An assumption of the Poisson model is equidispersion of the outcome, wherein the conditional variance equals the conditional mean. If this assumption is violated, we have overdispersion or underdispersion, and are likely to estimate spurious results.

We can check the dispersion in a fit Poisson model with `dispersiontest()`. This calculates the dispersion of the model by comparing the observed variability in the data to what the Poisson model would expect. Specifically, it examines the residuals (differences between observed and predicted values) to see if the data's spread is larger or smaller than anticipated.

-   `alternative` arguement: a character string specifying the alternative hypothesis: "greater" tests for overdispersion, "less" tests for underdispersion and "two.sided" tests for dispersion in either direction. Let's test for overdispersion, as this is more frequently encountered.

```{r}
dispersiontest(p1, alternative = "greater") # only works for poisson models
```

Interpretation: With `alternative` set to `"greater"`, a significant p-value suggests we are dealing with overdispersion.

Given that p-value is statistically significant, our model is likely overdispersed. Although our coefficient estimates may still be valid, the model’s standard errors are likely deflated, leading to potentially spurious results.

# Negative Binomial Model with GLM

In a Poisson model, overdispersion will bias SEs downward, leading to smaller p-values and a higher chance of spurious results and rejecting a true null hypothesis (type 1 error)\
By accounting for overdispersion Negative Binomial models will result in higher SEs.

To account for this overdispersion, we can estimate a negative binomial with the same function, specifying `negative.binomial` as the family.

To account for this overdispersion, we can estimate a negative binomial with the same function, specifying negative.binomial as the family. We have to set the theta (dispersion parameter) value. For moderate overdispersion, 1 is usually an appropriate value. For higher overdispersion, higher values might be appropriate. There are also ways of estimating theta from the data, but this often leads to convergance issues. Setting it to a specified value makes it easier for the optimization algorithm to estimate coefficients. 

```{r}
nb1 <- glm(number_of_victims ~ number_of_perpetrators +  scale(GDP) + 
                      GDP_Growth + Trade_Perc_GDP + Mineral_Rents_Perc, 
                      data = combo.df[combo.df$type_of_attack == 3,], 
                      family = negative.binomial(theta = 1))

summary(nb1)
stargazer(p1, nb1, type = "text")
```

## Model Comparison

```{r}
stargazer(p1, nb1, type = "text")
```

In this case, significance levels are unaffected, but we can see that standard errors do inflate going from the Poisson to the Negative Binomial.

# Zero-inflation model

For count variables with a high rate of zeros (more than 40-50% especially), a standard Poisson or Negative Binomial will be a poor fit and the model will underpredict zeros.

Here is some code for inspecting the number of zero and non-zero observations and the ratio of zeros:

```{r}

# Subset data to the type of attack used in the model
victim_counts <- table(combo.df[combo.df$type_of_attack == 3, ]$number_of_victims)

# Extract the count of zeros
num_zeros <- victim_counts["0"]

# Calculate the total count of non-zero observations
num_non_zeros <- sum(victim_counts) - num_zeros

# Calculate the ratio of zeros to total observations
ratio_zeros_non_zeros <- num_zeros / (num_zeros + num_non_zeros)

# Display the results
cat("Number of zero values:", num_zeros, "\n")
cat("Number of non-zero values:", num_non_zeros, "\n")
cat("Ratio of zeros to non-zeros:", ratio_zeros_non_zeros, "\n")

```

It looks like 56% of our observations are zeros.

With `pscl` package's `zeroinfl()` function we can fit a zero-inflated poisson or negative binomial model. The upper half is the poisson or negative binomial portion, while the bottom is the logit component.

There will be two sets of IVs in these models. The variables specified before the `|` will be those used to model the Poisson or Negative Binomial outcome. The variables specified after the `|` will be those used for the first-stage logit process, used to model excess zeros (zero-inflation).  

## Zero-inflation Poisson

```{r}
# Using zeroinfl function for zero-inflated regression model and negative binomial model
zip1 <- zeroinfl(number_of_victims ~ number_of_perpetrators + scale(GDP) + 
                   GDP_Growth + Trade_Perc_GDP + Mineral_Rents_Perc | number_of_perpetrators,
                 data = combo.df[combo.df$type_of_attack == 3,], 
                 dist = "poisson")

summary(zip1)
```

## Zero-inflation negative-binomial

```{r}

zib1 <- zeroinfl(number_of_victims ~ number_of_perpetrators + scale(GDP) + GDP_Growth + 
                      Trade_Perc_GDP + Mineral_Rents_Perc | number_of_perpetrators, 
                 data = combo.df[combo.df$type_of_attack == 3,], 
                 link = "logit", 
                 dist = "negbin")
```


# Plotting Predicted Counts with `ggeffects` 


```{r}
ggpredict(p1, terms = c("GDP")) %>% plot()

ggpredict(p1, terms = c("number_of_perpetrators")) %>% plot()
```

We can look at the raw values across levels of the specified variable by removing `%>% plot`

```{r}
ggpredict(p1, terms = c("GDP")) 

ggpredict(p1, terms = c("number_of_perpetrators")) 
```

By default, the other predictors are held at their mean or median values. We can specify certain levels as we have done with predicted probabilities and marginal effects.

```{r}
ggpredict(p1, terms = c("GDP", "number_of_perpetrators [400]")) 

ggpredict(p1, terms = c("GDP", "number_of_perpetrators [400]")) %>% plot

```

