---
title: "Linear Regression Review"
author: "Russ Luke, Updated by Ozlem Tuncel (Fall 2022) and Patrick Munger (Fall 2024)"
output:
  html_document: default
  pdf_document: default
---

```{r global setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This script reviews how to estimate a linear regression model in R using the lm() and glm() functions and how to implement checks for Gauss-Markov assumptions.

## Setup

You can set your working directory here, although since this script uses data from the "datasets" package rather than data imported from your local system, it should run without setting your working directory. It is also a good idea to set up an R project for this class to more easily manage your files and working directory.

This code chunk also sets a seed and loads the necessary libraries. Make sure they are all installed on your system first.

```{r setup, include=FALSE}
# Check your working directory
#getwd() 
# Specify the desired folder as your working directory
#setwd()

# Set seed for replication
set.seed(1234)

# Load libraries
library(datasets) # Where the data comes from
library(tidyverse) # Utility tools
library(lmtest) # Supplemental and post-estimation tests
library(sandwich) # Specific for the sandwich calculation of robust SE calculations
library(stargazer) # Create tables
library(car) # Variance Inflation Factors test
library(psych) # pairs plot
```

## Data Preparation

Here we load the state.x77 dataset from the datasets package and convert it into a dataframe where each row is a state and each column is a statistic.

```{r load data}
state_data <- data.frame(state.x77)
```

## Estimate Linear Models

### With lm()

First we estimate with R's function for linear models, lm()

```{r estimate with lm}
basic_OLS <- lm(Murder ~ Population + Income + Illiteracy, data = state_data)
summary(basic_OLS)
```

### With glm()

Now we estimate with glm(), R's function for estimating generalized linear models. The GLM generalizes the linear model to other distributions, which will become important in future weeks as we introduce non-linear models. However, a linear model can still be estimated with glm() by specifying the distribution or "family" as "gaussian" (or the normal distribution).

```{r estimate with glm}
mRate <- glm(Murder ~ Population + Income + Illiteracy, 
             family = "gaussian", 
             data = state_data)

summary(mRate)
```

Note that the results when estimating with lm() or glm() are the same. The only difference is lm() returns an R\^2 and adjusted R\^2 statistic while glm() returns the Log Likelihood and AIC, statistics which are used to assess non-linear model fit. We can compare the results in a stargazer table here.

```{r display results}
# Display in console
stargazer(basic_OLS, mRate, type = "text")

# Generate LaTeX code for table
#stargazer(basic_OLS, mRate, type = "latex")
```

## Check Gauss-Markov Assumptions

### Linearity of Relationship

We can create a QQ plot of the residuals to visually check the linearity of the relationship.

```{r qq plot}
qqnorm(residuals(mRate), ylab = "Residuals")
qqline(residuals(mRate))
```

### Distribution of Residuals

We can plot a histogram and calculate the standard deviation of the residuals to check their distribution and spread.

```{r residuals dist}
hist(mRate$residuals)
sd(mRate$residuals)
```

### Multicollinearity

To ensure that none of our independent variables are perfectly correlated, we can simply run a correlation test between each pair.

```{r corr test}
# Correlation between Population and Income
cor.test(state_data$Population, state_data$Income, 
         method = c("pearson"), 
         use = "complete.obs")

# Correlation between Population and Illiteracy
cor.test(state_data$Population, state_data$Illiteracy, 
         method = c("pearson"), 
         use = "complete.obs")

# Correlation between Income and Illiteracy
cor.test(state_data$Income, state_data$Illiteracy, 
         method = c("pearson"), 
         use = "complete.obs")
```

We can also plot and observe the relationship between all variables in the dataset.

```{r panels}
# Or alternatively, you can use the following:
par(mar = c(2, 2, 2, 2)) # set margins so plot fits 
pairs.panels(state_data)
```

Another way to test for multicollinearity between predictors in with a Variance Inflation Factor (VIF) test. You are safe from multicollinearity with values below 5. About 5 and there is likely a strong correlation between the predictor and another predictor. As VIF approaches 10 or exceeds 10, concern for multicollinearity becomes greater.

```{r VIF}
# Variable inflation factor 
# (Greater than 5 is an issue, greater than 10 is proof of multicollinearity)
vif(mRate)
```

### Spherical errors - residuals neither correlated with the independent variables nor one another

#### No serial/autocorrelation

To test that residuals are not correlated with eachother (serial or autocorrelation), we can perform a Durbin Watson test. The closer the DW statistic is to 2, the more autocorrelation is present. The p-value provides a way of categoricaly testing for autocorrelation at a given significance level, such as 0.05 (but remember p-values themselves are continuous probabilities, only the significance levels we decide on are categorical).

For the DW test, the null hypothesis is that there is no autocorrelation (independent residuals). So a p-value over 0.05 (p\>0.05) means we fail to reject the null and do not find significant concern for autocorrelation If the p-value is under 0.05 (p\<0.05), we reject the null, raising concern for autocorrelation.

```{r DW test}
dwtest(Murder ~ Population + Income + Illiteracy,
       data = state_data)
```

Here the p-value is 0.8759, meaning there is not significant concern for autocorrelation.

#### Heteroskedasticity

To visually assess if our residuals are heteroskedastic (non-constant variance across x values), we can start by plotting them against each of our predictors. This visualized observed versus predicted values.

```{r residual plots}
# Plot residuals vs Population variable
state_data %>% 
  ggplot(mapping = aes(y = residuals_lm, x = Population)) + 
  geom_point(col = 'blue') + 
  geom_abline(slope = 0)

# Plot residuals vs Income variable
state_data %>% 
  ggplot(aes(y = residuals_lm, x = Income)) + 
  geom_point(col = 'blue') + 
  geom_abline(slope = 0)

# Plot residuals vs Illiteracy variable
state_data %>% 
  ggplot(aes(y = residuals_lm, x = Illiteracy)) + 
  geom_point(col = 'blue') + 
  geom_abline(slope = 0)
```

A more objective way of testing for heteroskedasticity is with the Breusch-Pagan test. A higher BP statistic indicates stronger heteroskedasticity. Again, the p-value provides a way of categoricaly testing for heteroskedasticity at a given significance level.

The null hypothesis of the Breusch-Pagan test is that the residuals are homoskedastic (constant variance). So a p-value over 0.05 (p\>0.05) means we fail to reject the null and do not find significant concern for heteroskedasticity. If the p-value is under 0.05 (p\<0.05), we reject the null, raising concern for heteroskedasticity.

```{r BP test}
# Perform Breusch-Pagan test
bptest(mRate)
```

Here the p-value for our BP test is 0.04061, meaning we reject the null of homoskedastic residuals, a develop a concern that our residuals are heteroskedastic.

#### Robust Standard Errors

Often times heteroskedasticity is indicative of deeper patterns or issues in our data that need to be more holistically addressed before estimating a model and interpreting results. However, there is a "quick fix" for heteroskedasticity that is often used in applied research. Below, "Robust Standard Errors" are calculated to account for heteroskedasticity. This can either inflate or deflate the SEs (and thus p-values), depending on the nature of the heteroskedasticity. Note that coefficient estimates should not change. Also note, there are numerous ways of calculating robust or "heteroskedastistic consistent" (HC) standard errors. "HC1" is a slightly adjusted version of the original HC SEs proposed by White (1980). It will generally suffice.

```{r Robust SEs}
# Robust standard errors
coeftest(mRate, vcov = vcovHC(mRate, "HC1"))

# Calculate the 'robust' standard errors for stargazer function
cov_m <- vcovHC(mRate, method = "HC1")
rob_m <- sqrt(diag(cov_m))

## Presentation of Results
stargazer(mRate, 
          type = "text", 
          se = list(rob_m),
          ci = TRUE, df = FALSE, keep.stat = c("n"))
```
